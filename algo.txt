entries : n_images, n_process, images_size

Issues : 
    - cannot predict time spend on blur (examples : between 1 and 9 iterations)
    - cannot predict (can we?) distribution of processes per node (helps if balanced)

if n_process < n_images && images_size.are_same():
    static / dynamic allocation (scatter) per img

if n_process < n_images && image_size.differ_a_lot():
    cut images and worker pool per cut

if n_process > n_images && image_size.are_same():
    cut image in same number of parts
    worker_pool per cut 

if n_process > n_images && image_size.differ_a_lot():
    cut image weirdly
    worker_pool per cut


CALCUL DE GAIN SI COUPE
    - pp = ppcm(n_images, n_process) --> n_images / pp divisions
    ( min de divisions pour que chaque worker ait un travail Ã©quitable )


Sharing of ghost cells between workers ------ 


******************************GENERAL ALGO***************************************

Entries : animated image, n_process;

--> infos[], *pixels[]


for i in range(n_round):
    for j in n_img_per_round:
        scatter()
    
    work()

    for j in n_img_per_round()
        for k in part:
            recv()
            


Worker :
    While 1:
        - Check if incoming msg
        - If **STOP** 1: break
        - If new_part : alloc and receive

        apply_grey()

        while(iter)
            - apply_blur()
            - MPI_Allreduce() to see if another iteration --> if no : apply_sobel + send_to_root
            - if yes : 
                - send ghost_cells for left and right neighbors 
                - If need_ghost_cells_right && ghost_cells_right : update
                - If need_ghost_cells_left && ghost_cells_left : update

        apply_sobel()
        send_to_root()

        int MPI_Scatter(const void *sendbuf, int sendcount, MPI_Datatype sendtype,
               void *recvbuf, int recvcount, MPI_Datatype recvtype, int root,
               MPI_Comm comm)